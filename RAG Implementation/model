# from huggingface_hub import login

# login(token="LLAMA3_API_TOKEN")

import gradio as gr
import psycopg2
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import torch.nn.functional as F

embedding_model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer_embedding = AutoTokenizer.from_pretrained(embedding_model_ckpt)
model_embedding = AutoModel.from_pretrained(embedding_model_ckpt)

llama_model_ckpt = "meta-llama/Meta-Llama-3-8B"
tokenizer_llama = AutoTokenizer.from_pretrained(llama_model_ckpt)
model_llama = AutoModelForCausalLM.from_pretrained(llama_model_ckpt)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_embedding.to(device)
model_llama.to(device)

connection = psycopg2.connect(
    dbname="vector_db", 
    user="postgres",
    password="root123",
    host="localhost",
    port="5432"
)

def create_embedding(text):
    encoded_input = tokenizer_embedding(text, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        model_output = model_embedding(**encoded_input)
    input_mask = encoded_input['attention_mask']
    token_embeddings = model_output.last_hidden_state
    input_mask_expanded = input_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return F.normalize(sum_embeddings / sum_mask, p=2, dim=1)

def retrieve_most_similar(query_text, top_k=5):
    query_embedding = create_embedding(query_text).cpu().numpy()[0]
    cursor = connection.cursor()
    
    cursor.execute("SELECT filename, chunk_index, embedding FROM chunks;")
    db_embeddings = cursor.fetchall()

    similarities = []
    for filename, chunk_index, embedding_str in db_embeddings:
        db_embedding = np.array(eval(embedding_str.replace('{', '[').replace('}', ']')))
        similarity = np.dot(query_embedding, db_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(db_embedding))
        similarities.append((similarity, filename, chunk_index))

    similarities.sort(reverse=True)
    top_results = similarities[:top_k]

    result_metadata = []
    for similarity, filename, chunk_index in top_results:
        cursor.execute("""
            SELECT title, authors, doi
            FROM metadata
            WHERE filename = %s
        """, (filename,))
        metadata = cursor.fetchone()
        result_metadata.append((metadata, chunk_index, similarity))

    cursor.close()
    return result_metadata

def generate_response(prompt):
    retrieval_results = retrieve_most_similar(prompt)
    contexts = [f"Title: {meta[0][0]}\nAuthors: {meta[0][1]}\nDOI: {meta[0][2]}\nChunk Index: {meta[1]}\nSimilarity: {meta[2]}" for meta in retrieval_results]
    combined_input = prompt + "\n" + "\n".join(contexts)
    
    inputs = tokenizer_llama(combined_input, return_tensors="pt").to(device)
    outputs = model_llama.generate(**inputs, max_length=512)
    response = tokenizer_llama.decode(outputs[0], skip_special_tokens=True)
    return response

def gradio_interface(prompt):
    return generate_response(prompt)

interface = gr.Interface(
    fn=gradio_interface,
    inputs="text",
    outputs="text",
    title="RAG Model with LLaMA3",
    description="Enter your query and get a response generated by LLaMA3 based on the most relevant information from the database."
)

if __name__ == "__main__":
    interface.launch()
